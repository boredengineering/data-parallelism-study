{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Neural Network Optimization\n",
    "\n",
    "Welcome to the first lab of the course. The goal of this exercise is threefold:\n",
    "\n",
    "- Review some basic concepts of neural network training and establish a common vocabulary for the class.\n",
    "- Illustrate the basic principles involved in training of a neural network (in particular, stochastic gradient descent).\n",
    "- Create a foundation for the next labs in this class that will eventually lead us to a multi-GPU implementation of a neural network.\n",
    "\n",
    "We will start with the simplest possible neural network, a single linear neuron:\n",
    "\n",
    "<img src=\"https://developer.download.nvidia.com/training/images/C-MG-01-V1_task1_img_LinearNeuron.png\" width=\"300\" height=\"300\"/> \n",
    "\n",
    "We will illustrate how this neural network can be trained using gradient descent and stochastic gradient descent algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network\n",
    "\n",
    "### Generating a random dataset\n",
    "\n",
    "In this exercise we will train our neural network to fit a randomly generated dataset representing a line with random noise added to it. We are choosing to use a simple neural network that matches this dataset: although the equation for a line cannot exactly match the data, due to the noise, it is a very good approximation that still allows us to dig into the neural network training process.\n",
    "\n",
    "We start by importing the necessary Python libraries. Since this exercise is deliberately simple, the list is quite short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPcUrE5PuoJX"
   },
   "outputs": [],
   "source": [
    "# Numpy is a fundamental package for scientific computing. It contains an implementation of an array\n",
    "# that we will use in this exercise.\n",
    "import numpy as np\n",
    "# We will be generating our own random dataset. As a consequence we need functionality to generate random numbers.\n",
    "import random\n",
    "# We will be plotting the progress of training using matplotlib, a package that can be used to generate 2D and 3D plots.\n",
    "# We use the \"widget\" option to enable interactivity later on.\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "# We will use PyTorch as the deep learning framework of choice for this class.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBumKenbuoqf"
   },
   "outputs": [],
   "source": [
    "# Define the number of samples/data points you want to generate\n",
    "n_samples = 100\n",
    "# We will define a dataset that lies on a line as defined by y = w_gen * x + b_gen\n",
    "w_gen = 10\n",
    "b_gen = 2\n",
    "# To make the problem a bit more interesting we will add some Gaussian noise as \n",
    "# defined by the mean and standard deviation below.\n",
    "mean_gen = 0\n",
    "std_gen = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ1_yWSgux5g"
   },
   "outputs": [],
   "source": [
    "# This section generates the training dataset as defined by the variables in the section above.\n",
    "x = np.float32(np.random.uniform(0, 10, [n_samples, 1]))\n",
    "y = np.float32(np.array([w_gen * (x + np.random.normal(loc=mean_gen, scale=std_gen, size=None)) + b_gen for x in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "0HdPAalowR8C",
    "outputId": "e052e9a8-5d29-4328-932f-76b1378fd84d"
   },
   "outputs": [],
   "source": [
    "# Plot our randomly generated dataset\n",
    "plt.close()\n",
    "plt.plot(x, y, 'go')\n",
    "plt.xlabel(\"x\", size=24)\n",
    "plt.ylabel(\"y\", size=24)\n",
    "plt.tick_params(axis='both', labelsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the model\n",
    "\n",
    "Regardless of the complexity of the machine learning problem the process of code development consists of:\n",
    "\n",
    "- Creating a definition of the model\n",
    "- Defining the loss (cost) function that will guide our training process. The loss function is effectively a definition of success that informs our optimization algorithm about the progress made during training. </li>\n",
    "- Then iteratively:\n",
    "  - Calculating the gradient of the loss function with respect to the model weights.\n",
    "  - Updating the model weights (in the direction opposite to the gradient) to minimize the loss function.\n",
    "\n",
    "Let's implement the above for our simple model. We start by defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIXgyJcOwTtO"
   },
   "outputs": [],
   "source": [
    "# Convert our inputs and outputs to Tensors. Storing our data as Tensors enables us to perform computations on the GPU. \n",
    "x = torch.from_numpy(x) if not torch.is_tensor(x) else x\n",
    "y = torch.from_numpy(y) if not torch.is_tensor(y) else y\n",
    "\n",
    "# To ensure that our code is device agnostic, we set our compute platform to be GPU based if one is available and CPU otherwise.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# There are several ways to implement our simple model. We are using PyTorch's built-in Linear module to initialize a model of the form y = w*x + b, \n",
    "# where w (weight) and b (bias) are the parameters to be learned. \n",
    "model = torch.nn.Linear(1, 1, bias = True, device = device)\n",
    "\n",
    "# We will initialize w and b to be 0.\n",
    "with torch.no_grad():\n",
    "    model.weight[0] = 0\n",
    "    model.bias[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function\n",
    "\n",
    "We now must define what we mean by success. There exists a wide range of measures of success (loss functions) that can be used by neural networks. For more details on the range of loss function you can use, and a detailed explanation of how to make this choice, refer to section 6.2.1 of the <a href=\"http://www.deeplearningbook.org/\">Deep Learning Book</a> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n",
    "\n",
    "In our case we will use a simple definition of success. We will measure the mean of the <b>squared distance</b> of all the points in the dataset from the straight line we are trying to find. \n",
    "\n",
    "<img src=\"https://developer.download.nvidia.com/training/images/C-MG-01-V1_task1_img_CostFunction.png\" width=\"350\" height=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the loss function which is an indicator of how good or bad our model is at any point of time.\n",
    "loss_fn = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the optimization logic: gradient descent\n",
    "\n",
    "Having defined the model as well as the loss function, the next step is to choose the optimization algorithm we will use to find the combination of parameters **w** and **b** that minimize our loss function (and thus give us the best performance). There exists a wide range of optimization algorithms to choose from (for a more detailed discussion refer to chapter 8 of the [Deep Learning Book](http://www.deeplearningbook.org/contents/optimization.html)). In this exercise we will use one of the most basic optimization algorithms, **gradient descent**. The mechanism by which gradient descent operates is illustrated in the figure below (where `b` is not shown, for simplicity. Bear in mind that for non-convex functions, which applies to the majority of neural networks, the algorithm may end up converging on a good local minimum instead of a global one:\n",
    "\n",
    "<img src='https://developer.download.nvidia.com/training/images/C-MG-01-V1_task1_img_GradientDescent.png'/>\n",
    "\n",
    "At every step of the process, the model runs through the full dataset using the current value of the model parameters (in our case w and b) and calculates the loss. Then the gradient of the loss function is calculated (in this simple case, it is the slope of the line tangent to the curve). Once the gradient is calculated it can be used to slowly move towards the optimal solution.\n",
    "\n",
    "In practice gradient descent (or even the stochastic gradient descent method discussed below) is rarely used directly. Instead, more effective variants exist that allow the algorithm to find the solution faster and provide better stability during computation. Note also that it is rarely the case that the gradient calculation and optimization logic must be written from scratch. Instead, all the key deep learning frameworks provide auto differentiation as well as a wide range of optimization algorithms. In our case we will choose a built-in implementation of gradient descent provided by the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xinhifdl61tg"
   },
   "outputs": [],
   "source": [
    "# Define a gradient descent optimizer\n",
    "# Note that the \"SGD\" optimizer is simple gradient descent if applied\n",
    "# to the full dataset, and stochastic gradient descent if applied to\n",
    "# random subsets of the dataset\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "We have now defined our dataset, model, loss function, and optimization algorithm and are ready to start the training (optimization) process. The loop presented below will use all our training data to compute the gradient of the loss function with respect to the model parameters. The optimizer called in the loop will then make small changes to the model parameters, bringing it closer and closer to our desired solution. (The size of the change in each step is determined by the learning rate that we defined earlier.) We will repeat the process enough times to reach a reasonable solution. Often the way to know that you have reached a good stopping point is that the loss function has ceased to decrease.\n",
    "\n",
    "The goal of this exercise is to understand how certain properties of the optimization process depend on the gradient descent method (specifically, on the batch size). To demonstrate that we will be logging the value of the loss function as we train the network and then visualize it.\n",
    "\n",
    "We've asked you to complete a small task in the below code, indicated by the `FIXME` in the cell below (the code will not run as is). The code block below trains for a maximum of 1000 epochs, which is much more than is needed for this problem. Write code inside the training loop that implements early stopping, that is, exits the loop when the training has converged. There is no universal definition of convergence, so you'll have to pick one that is appropriate for this problem. One possible choice is to stop training when the loss function changes by less than 0.1% between epochs (consider testing over a suitable average of previous epochs.) You could also choose to consider how fast the model parameters are changing. If you get stuck, you can always remove the convergence check and control the training process by modifying `max_number_of_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaLGZlIG24Di"
   },
   "outputs": [],
   "source": [
    "# The following line will reset model parameters so that the model is retrained from scratch every time,\n",
    "# in case you need to re-run this cell during your work.\n",
    "model.reset_parameters()\n",
    "\n",
    "# Define the maximum number of times we want to process the entire dataset (the number of epochs).\n",
    "# In practice we won't run this many because we'll implement an early stopping condition that\n",
    "# detects when the training process has converged.\n",
    "max_number_of_epochs = 1000\n",
    "\n",
    "# We will store information about the optimization process here.\n",
    "loss_array = []\n",
    "b_array = []\n",
    "w_array = []\n",
    "\n",
    "# We will move our inputs and outputs to the GPU, where the model is also located.\n",
    "inputs = x.to(device)\n",
    "outputs = y.to(device)\n",
    "\n",
    "# Print out the parameters and loss before we do any training\n",
    "outputs_predicted = model(inputs)\n",
    "loss_value = loss_fn(outputs_predicted, outputs)\n",
    "print(\"Before training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(model.weight.item(), model.bias.item(), loss_value))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Starting training\")\n",
    "print(\"\")\n",
    "\n",
    "# Start the training process\n",
    "for i in range(max_number_of_epochs):\n",
    "    \n",
    "    # Explicitly set gradients to zero before each forward pass to avoid gradient accumulation.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Generate outputs for each example in our dataset and compute loss\n",
    "    outputs_predicted = model(inputs)\n",
    "    loss = loss_fn(outputs_predicted, outputs)\n",
    "\n",
    "    # Compute gradient of the loss with respect to w and b\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Capture the data that we will use in our visualization\n",
    "    w_array.append(model.weight.item())\n",
    "    b_array.append(model.bias.item())\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Epoch = {:2d}: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(i+1, model.weight.item(), model.bias.item(), loss.item()))\n",
    "\n",
    "    # Implement your convergence check here, and exit the training loop if\n",
    "    # you detect that we are converged:\n",
    "    if FIXME:\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training finished after {} epochs\".format(i+1))\n",
    "print(\"\")\n",
    "\n",
    "print(\"After training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(model.weight.item(), model.bias.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck with the above exercise, reveal the cell below (by clicking on the three dots) to see an example convergence check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font face=\"courier\">\n",
    "<pre>\n",
    "            # Example solution for the convergence check<br />\n",
    "            if i > 1 and abs(loss_array[i] - loss_array[i-1]) / loss_array[i-1] < 0.001:<br />\n",
    "</pre>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output listed below we can see that we have managed to minimize our loss by a very large amount and managed to obtain a solution reasonably close to the expected function. (Compare the current values of w and b with the target values, w_gen and b_gen.) Now let's plot the loss as a function of time (number of completed epochs). This plot is fundamental for monitoring the progress of the training process, and helps us understand how to make decisions related to model or dataset improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel(\"Epoch\", size=24)\n",
    "plt.ylabel(\"Loss\", size=24)\n",
    "plt.tick_params(axis='both', labelsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the progress of the loss function\n",
    "\n",
    "Since in our case the loss function depends on only two parameters (w and b) it is possible to directly visualize its shape. Moreover, it is possible to visualize the trajectory that our optimization algorithm took in this loss function space. The below plot illustrates just that (note that this plot is interactive, and you can rotate it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(w_array, b_array, loss_array)\n",
    "\n",
    "ax.set_xlabel('w', size=16)\n",
    "ax.set_ylabel('b', size=16)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extend our visualization by plotting it against the entire loss function in this region. Since the entire dataset is used for the computation of the loss function we obtain just one plane, and the trajectory that our optimizer takes is fairly smooth with almost no noise. This will not be the case when we start working with subsets of the data using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZhyVMX_8Xjz"
   },
   "outputs": [],
   "source": [
    "loss_surface = []\n",
    "w_surface = []\n",
    "b_surface = []\n",
    "\n",
    "for w_value in np.linspace(0, 20, 200):\n",
    "    for b_value in np.linspace(-18, 22, 200):\n",
    "        \n",
    "        # Collect information about the loss function surface\n",
    "        with torch.no_grad():\n",
    "            model.weight[0] = w_value\n",
    "            model.bias[0] = b_value\n",
    "\n",
    "        outputs_predicted = model(inputs)\n",
    "        loss = loss_fn(outputs_predicted, outputs)\n",
    "        \n",
    "        b_surface.append(b_value)\n",
    "        w_surface.append(w_value)\n",
    "        loss_surface.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "YObSqSrS8Ztm",
    "outputId": "e04cbd1f-98a7-419c-e3d5-8e908445920a"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax2.scatter(w_surface, b_surface, loss_surface, c = loss_surface, alpha = 0.02)\n",
    "ax2.plot(w_array, b_array, loss_array, color='black')\n",
    "\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "In contrast to gradient descent, stochastic gradient descent does not use the entire dataset for the calculation of the shape of the loss function. Instead, smaller subsets of the data are used. This has profound implications for the behavior of our algorithm. Since every batch is different, even for the same set of weights, the calculated loss will be different for an individual batch than for the entire dataset. Since the loss function is different for the batch, the gradient of the loss function will be different as well, which introduces a level of noise.\n",
    "\n",
    ">NOTE: Formally, **stochastic** gradient descent denotes the use of a batch size of 1, **batch** gradient descent denotes using the entire dataset as a single batch, and **mini-batch** gradient descent denotes batch sizes anywhere in between **stochastic** and **batch** gradient descent. That said, in common parlance, we don't often distinguish between **stochastic** and **mini-batch** gradient descent and use the term **stochastic** gradient descent to mean any batch size smaller than the entire dataset.\n",
    "\n",
    "Let's think about how stochastic gradient descent works as applied to our toy model. The thick green line illustrates the shape of the loss function given all the data. The thin green line is the shape of the loss function for an individual batch (sometimes called a mini-batch). Since those curves are different the estimation of the gradient will be different at every step. This can be seen as adding an element of noise to the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "![](./images/SGD1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "![](./images/SGD2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "![](./images/SGD3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding an element of noise in this process (the smaller the batch size, the larger the noise), we can reduce the likelihood of getting stud in a local minima and increase the chance of locating the global minima. Later we will be investigating later how this plays out in the context of a realistic neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing stochastic gradient descent: a first approach\n",
    "\n",
    "To demonstrate this phenomenon let's make a small change to our code. Rather than providing all the data to the model in every iteration, we will provide just a single example (batch size of 1), amplifying the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWqK0I2C_cpX",
    "outputId": "68ccbb09-18a0-4f3f-8d1d-1999a3a1d547"
   },
   "outputs": [],
   "source": [
    "# The following line will reset model parameters so that the model is retrained from scratch every time,\n",
    "# in case you need to re-run this cell during your work.\n",
    "model.reset_parameters()\n",
    "\n",
    "# Define the maximum number of times we want to process the entire dataset (the number of epochs).\n",
    "# In practice we won't run this many because we'll implement an early stopping condition that\n",
    "# detects when the training process has converged.\n",
    "max_number_of_epochs = 1000\n",
    "\n",
    "# We will store information about the optimization process here.\n",
    "loss_array = []\n",
    "b_array = []\n",
    "w_array = []\n",
    "\n",
    "# Zero out the initial values\n",
    "with torch.no_grad():\n",
    "    model.weight[0] = 0\n",
    "    model.bias[0] = 0\n",
    "    \n",
    "# Print out the parameters and loss before we do any training\n",
    "outputs_predicted = model(inputs)\n",
    "loss_value = loss_fn(outputs_predicted, outputs)\n",
    "print(\"Before training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(model.weight.item(), model.bias.item(), loss_value))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Starting training\")\n",
    "print(\"\")\n",
    "\n",
    "# Start the training process\n",
    "for i in range(max_number_of_epochs):\n",
    "    \n",
    "    # Update after every data point\n",
    "    for (x_pt, y_pt) in zip(inputs, outputs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pt_predicted = model(x_pt)\n",
    "\n",
    "        loss = loss_fn(y_pt_predicted, y_pt)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Capture the data that we will use in our visualization\n",
    "        # Note that we are now updating our loss function after\n",
    "        # every point in the sample, so the size of loss_array\n",
    "        # will be greater by a factor of n_samples compared to\n",
    "        # the last exercise.\n",
    "        w_array.append(model.weight.item())\n",
    "        b_array.append(model.bias.item())\n",
    "        loss_array.append(loss.item())\n",
    "\n",
    "    # At the end of every epoch after the first, print out the learned weights\n",
    "    if i > 0:\n",
    "        avg_w = sum(w_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "        avg_b = sum(b_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "        avg_loss = sum(loss_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "        print(\"Epoch = {:2d}: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(i+1, avg_w, avg_b, avg_loss))\n",
    "\n",
    "    # End the training when the loss function has not changed from the last epoch\n",
    "    # by more than a small amount. Note that in our convergence check we will compare\n",
    "    # the loss averaged over this epoch with the loss averaged over the last epoch.\n",
    "    if i > 1:\n",
    "        average_loss_this_epoch = sum(loss_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "        average_loss_last_epoch = sum(loss_array[(i-2)*n_samples:(i-1)*n_samples]) / n_samples\n",
    "        if abs(average_loss_this_epoch - average_loss_last_epoch) / average_loss_last_epoch < 0.001:\n",
    "            break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training finished after {} epochs\".format(i+1))\n",
    "print(\"\")\n",
    "\n",
    "avg_w = sum(w_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "avg_b = sum(b_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "avg_loss = sum(loss_array[(i-1)*n_samples:(i  )*n_samples]) / n_samples\n",
    "\n",
    "print(\"After training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(avg_w, avg_b, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the final loss value to what we obtained before, as well as the model parameters. You probably did not get the same value for the total loss -- was the answer any more accurate?\n",
    "\n",
    "Let's plot the loss over time. One choice we can make is to plot the loss after every update. This will probably be a significantly noisier curve, since we aren't averaging out updates over an entire pass through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "4pqN-Czx9LjI",
    "outputId": "f7a75d85-1526-4f9e-ca29-6544b06f54c0"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel(\"Number of Updates\", size=24)\n",
    "plt.ylabel(\"Loss\", size=24)\n",
    "plt.tick_params(axis='both', labelsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a similar effect by plotting the trajectory taken by our optimizer in the loss space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "HVBahMjY33EO",
    "outputId": "7d8454ec-f040-4ba6-9c2b-d55a1bb73326"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(w_array, b_array, loss_array)\n",
    "\n",
    "ax.set_xlabel('w', size=16)\n",
    "ax.set_ylabel('b', size=16)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>Plotting the loss surface for one of the mini batches clearly illustrates that it no longer aligns with the optimization trajectory which was generated one minibatch at a time (and for every minibatch the loss surface was indeed different).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "0pknNKsJ4Zok",
    "outputId": "006a8c5e-c37d-4623-a903-06a57652b565"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax2.scatter(w_surface, b_surface, loss_surface, c = loss_surface, alpha = 0.02)\n",
    "ax2.plot(w_array, b_array, loss_array, color='black')\n",
    "\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with batch size in this toy model\n",
    "\n",
    "So, we've now compared two extremes: gradient descent where the batch size is the full dataset (that is, traditional gradient descent), and SGD where the batch size is 1. Let's try out some batch sizes in the middle. To do this, use the cell below to modify the code we used above to pass a subset of the data to be trained on in each step.\n",
    "\n",
    "All you need to do is loop through the dataset in order, passing each consecutive chunk of batch size M to the optimizer. Your implementation ideally should work for any number M where 1 <= M <= N, where N is the number of items in the dataset. The dataset will usually not be evenly divisible by the size of the batch, so make sure you handle the last batch in the dataset by just using all the remaining items (so it will be a smaller batch).\n",
    "\n",
    "There are 3 `FIXME` statements in the cell below to guide your work, as well as a solution below, if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b71yjQwI4eEg",
    "outputId": "68922f5f-d957-4dc1-a3b4-7b0481d39b85"
   },
   "outputs": [],
   "source": [
    "# The following line will reset model parameters so that the model is retrained from scratch every time,\n",
    "# in case you need to re-run this cell during your work.\n",
    "model.reset_parameters()\n",
    "\n",
    "# Define the maximum number of times we want to process the entire dataset (the number of epochs).\n",
    "# In practice we won't run this many because we'll implement an early stopping condition that\n",
    "# detects when the training process has converged.\n",
    "max_number_of_epochs = 1000\n",
    "\n",
    "# We will store information about the optimization process here.\n",
    "loss_array = []\n",
    "b_array = []\n",
    "w_array = []\n",
    "\n",
    "# Zero out the initial values\n",
    "with torch.no_grad():\n",
    "    model.weight[0] = 0\n",
    "    model.bias[0] = 0\n",
    "    \n",
    "# Print out the parameters and loss before we do any training\n",
    "outputs_predicted = model(inputs)\n",
    "loss_value = loss_fn(outputs_predicted, outputs)\n",
    "print(\"Before training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(model.weight.item(), model.bias.item(), loss_value))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Starting training\")\n",
    "print(\"\")\n",
    "\n",
    "# Pass in batches of the dataset\n",
    "# After the first run, try batch sizes of 16, 64, and 128\n",
    "batch_size = 32\n",
    "num_batches_in_epoch = FIXME\n",
    "\n",
    "# Start the training process\n",
    "for i in range(max_number_of_epochs):\n",
    "\n",
    "    for j in range(num_batches_in_epoch):\n",
    "        batch_start = FIXME\n",
    "        batch_end = FIXME\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_predicted = model(inputs[batch_start:batch_end])\n",
    "\n",
    "        loss = loss_fn(outputs_predicted, outputs[batch_start:batch_end])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        w_array.append(model.weight.item())\n",
    "        b_array.append(model.bias.item())\n",
    "        loss_array.append(loss.item())\n",
    "\n",
    "    # At the end of every epoch after the first, print out the learned weights\n",
    "    if i > 0:\n",
    "        avg_w = sum(w_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "        avg_b = sum(b_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "        avg_loss = sum(loss_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "        print(\"Epoch = {:2d}: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(i+1, avg_w, avg_b, avg_loss))\n",
    "\n",
    "    # End the training when the loss function has not changed from the last epoch\n",
    "    # by more than a small amount. Note that in our convergence check we will compare\n",
    "    # the loss averaged over this epoch with the loss averaged over the last epoch.\n",
    "    if i > 1:\n",
    "        average_loss_this_epoch = sum(loss_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "        average_loss_last_epoch = sum(loss_array[(i-2)*num_batches_in_epoch:(i-1)*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "        if abs(average_loss_this_epoch - average_loss_last_epoch) / average_loss_last_epoch < 0.001:\n",
    "            break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training finished after {} epochs\".format(i+1))\n",
    "print(\"\")\n",
    "\n",
    "avg_w = sum(w_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "avg_b = sum(b_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "avg_loss = sum(loss_array[(i-1)*num_batches_in_epoch:(i  )*num_batches_in_epoch]) / num_batches_in_epoch\n",
    "\n",
    "print(\"After training: w = {:4.3f}, b = {:4.3f}, loss = {:7.3f}\".format(avg_w, avg_b, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck with the above exercise, reveal the cell below to see an example solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<font face=\"courier\">\n",
    "<pre>\n",
    "\n",
    "num_batches_in_epoch = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "...\n",
    "\n",
    "    for j in range(num_batches_in_epoch):\n",
    "        batch_start = j * batch_size\n",
    "        batch_end = min((j + 1) * batch_size, n_samples)\n",
    "\n",
    "</pre>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each batch size you try, repeat the plotting process to see how things went. Note which batch size yielded the best final accuracy, but in particular pay attention to the smoothness of the accuracy curve as a function of epoch count, and consider how that smoothness depends on batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "TOOPS4fd50Fg",
    "outputId": "15c5707a-ea4d-49a0-8ac0-163b311aae79"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel(\"Number of Updates\", size=24)\n",
    "plt.ylabel(\"Loss\", size=24)\n",
    "plt.tick_params(axis='both', labelsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "8slmLRtC527-",
    "outputId": "2b6758d4-1bb1-4234-c718-8c0f59321ee8"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(w_array, b_array, loss_array)\n",
    "\n",
    "ax.set_xlabel('w', size=16)\n",
    "ax.set_ylabel('b', size=16)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "A3mpuCpz6Byu",
    "outputId": "6d704c94-0158-45d0-d6a3-df30cf489cab"
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax2.scatter(w_surface, b_surface, loss_surface, c = loss_surface, alpha = 0.02)\n",
    "ax2.plot(w_array, b_array, loss_array, color='black')\n",
    "\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "7sSp65Yk6Drx",
    "outputId": "c2497fc2-3289-47fa-b6a6-27e2250cbfbe"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this task we have learned:\n",
    "\n",
    "- The basic components of the neural network training process\n",
    "- How to implement a gradient descent optimizer\n",
    "- The difference between gradient descent and stochastic gradient descent and the impact they have on the optimization process\n",
    "- How batch size affects training accuracy\n",
    "\n",
    "In the next part of this lab we will take the code developed above and expand it to a larger neural network so that we can eventually demonstrate a multi-GPU implementation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
