{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa8f90-a82c-4c5f-85cb-9d39fe812068",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392afb87-4365-4403-809d-c53665ed29b7",
   "metadata": {},
   "source": [
    "Please execute the cell below to view the slide presentation that the instructor will present at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9672eb-435a-4052-8608-b83e325f3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://view.officeapps.live.com/op/embed.aspx?src=https://developer.download.nvidia.com:443/training/courses/C-MG-01-V3/task1/2-v1.pptx\" width=\"610px\" height=\"367px\" frameborder=\"0\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://view.officeapps.live.com/op/embed.aspx?src=https://developer.download.nvidia.com:443/training/courses/C-MG-01-V3/task1/2-v2.pptx\" width=\"610px\" height=\"367px\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4886a3b-deb5-4fc2-8ed1-b319af4c3532",
   "metadata": {},
   "source": [
    "# A more realistic model\n",
    "\n",
    "So far you've learned about the difference between gradient descent (i.e. a batch size equal to the full dataset) and mini-batch stochastic gradient descent (the batch size is smaller than the full dataset, usually much smaller). You've seen that smaller batch sizes add noise to the optimization process, which can help with avoiding getting trapped in local minima or slowed down at a saddle point. A smaller batch size will also mean running back-propagation and updating the gradients more times per epoch (taking more steps).\n",
    "\n",
    "In this lab you'll be experimenting with batch size on a more complex dataset and model. You will see the effect of batch size on GPU performance, as well as on the accuracy of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac43b0-220b-49be-aeeb-2e9385404312",
   "metadata": {},
   "source": [
    "## The Fashion-MNIST Dataset\n",
    "\n",
    "The [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) is a response to the traditional MNIST dataset, which is often referred to as the \"hello world\" of machine learning. The original MNIST dataset consists of 60,000 pictures of handwritten digits, 0-9. One of the downsides of this dataset is its simplicity. Good performance of a model on the dataset does not indicate that the model will perform well on a more complicated set of images.\n",
    "\n",
    "The Fashion-MNIST dataset was created to be a moderately more complex image classification challenge. It follows the same format as the original MNIST set, with 10 categories and 60,000 images, each 28x28 pixels (plus 10,000 testing images). We'll be training on this dataset for this exercise, as well as for the next labs where we'll introduce training with multiple GPUs. \n",
    "\n",
    "<img src=\"./images/Fashion MNIST.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461e540-2e10-47b2-b7a2-51c7f24012b0",
   "metadata": {},
   "source": [
    "## The Wide ResNet Model\n",
    "\n",
    "We'll be using a Wide Residual Network to train on this dataset, which is a convolutional neural network proven to perform very well in image classification challenges. Feel free to take some time to learn more about [wide residual networks](https://arxiv.org/abs/1605.07146), the original [residual networks](https://arxiv.org/abs/1512.03385) they are based on, or about [convolutional neural networks](https://developer.nvidia.com/discover/convolutional-neural-network) in general.\n",
    "\n",
    "![wideresnet](./images/wideresnet.png)\n",
    "\n",
    "In the early days of CNNs, the community drove towards very deep models (many tens or hundreds of layers), but as computing power advanced and algorithms improved, in particular after the idea of the residual block was demonstrated, it became more desirable to swing back towards shallower networks with wider layers, which was the primary innovation of the WideResNet family of models. The WideResNet-16-10 we will use below can achieve with O(10 million) parameters accuracy that is competitive with much deeper networks with more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c761fd-ec03-471b-89cf-906d550a99ca",
   "metadata": {},
   "source": [
    "## Training our Model\n",
    "\n",
    "We'll start by running training on the existing dataset with default hyperparameters. Please take a few minutes to look through `fashion_mnist.py`, and get familiar with the training. We're using PyTorch for this training, but the takeaways of these exercises should translate to other frameworks.\n",
    "\n",
    "Notice that we're only training on 1/6 of the dataset (10,000 images) for this exercise. We're doing this to keep epoch times short so that we can run quick experiments and see the effects of batch size. When we start introducing multiple GPUs to speed up the training, we'll use the entire dataset.\n",
    "\n",
    "Once you have a good sense of the code, execute the cell below to run a few epochs. Pay attention to the validation accuracy, validation loss, and epoch time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ac986-d161-4b95-aeb9-b786a1dc9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --epochs 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688db8a-d454-4e81-8f10-815b71d5c3a2",
   "metadata": {},
   "source": [
    "We are about to make some edits to this file, so let's make a copy of this file that you can refer to or back up to if you make any mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dc398-fd42-49f4-8665-fa95eb185c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp fashion_mnist.py fashion_mnist_original.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c971a19-143c-4f02-9e85-ba76d7ae8cb5",
   "metadata": {},
   "source": [
    "## Training Performance - Images/Second\n",
    "\n",
    "One way of measuring the performance of our training is how much data is being processed for a given unit of time. GPUs are highly optimized for parallel processing, and many aspects of the training process utilize this parallelism. Take a moment to think about why batch size might have an effect on the GPUs ability to parallelize, and what might happen to the performance. \n",
    "\n",
    "In this exercise, you'll implement the feature to report how many images are processed each second as the neural network trains. You'll then adjust the batch size to see empirically how performance (or throughput) is affected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d8d31-97ef-496d-ac69-d8ceb4fea394",
   "metadata": {},
   "source": [
    "### Computing Image Throughput\n",
    "\n",
    "PyTorch gives us complete control over the training loop. We'll be making use of this flexibility to compute and report images/second throughput for each epoch. Take a moment to look at `TODO Step 1` locations in `fashion_mnist.py` and implement the image throughput calculation.\n",
    "\n",
    "Once you've completed the implementation, execute the training again and pay attention to the image throughput.\n",
    "\n",
    "If you get stuck, feel free to reference [solutions/fashion_mnist_after_step_01.py](solutions/fashion_mnist_after_step_01.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68382be-9d48-4d7d-9eef-7b6da6d59c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c811b-dda3-4e85-9f81-3223a95d12bf",
   "metadata": {},
   "source": [
    "You'll notice that the throughput increases after the first epoch. This can be attributed to one time costs such as data loading and memory allocation. For the next exercise, just focus on the throughput in the second epoch and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711fdeb-e3d5-4d1b-aa66-f3d2e6d53afe",
   "metadata": {},
   "source": [
    "### Comparing Throughput by Batch Size\n",
    "\n",
    "In this exercise you'll be calculating training throughput as a function of batch size. Execute the next cell several times as you adjust the batch size. Enter the data in the cell below (replace each \"FIXME\" with the images/sec throughput for the corresponding batch size), and execute it to see a plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e56b20-3337-40c4-984d-d1cc35e40eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --epochs 5 --batch-size FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140eaf9-53c3-4b97-83df-164d92f54eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [('8', FIXME),\n",
    "        ('16', FIXME),\n",
    "        ('32', FIXME),\n",
    "        ('64', FIXME),\n",
    "        ('128', FIXME),\n",
    "        ('256', FIXME),\n",
    "        ('512', FIXME),\n",
    "        ('700', FIXME)]\n",
    "\n",
    "x,y = zip(*data)\n",
    "plt.bar(x,y)\n",
    "plt.ylabel(\"Throughput (images / sec)\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c2516-9ea7-46b9-a692-67f48e96dc94",
   "metadata": {},
   "source": [
    "If you don't want to manually find each datapoint, you can reveal the code block below and copy over the provided data. These values may vary slightly from what you may have produced with your own code, but were generated on the same compute environment as you are currently using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd97bd1-174b-4dab-9e5d-a92d150b7e3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "data = [('8', 322),\n",
    "        ('16', 485),\n",
    "        ('32', 575),\n",
    "        ('64', 628),\n",
    "        ('128', 652),\n",
    "        ('256', 657),\n",
    "        ('512', 680),\n",
    "        ('700', 683)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f0b44-0082-4f8a-a64e-ac6184640dcb",
   "metadata": {},
   "source": [
    "Take some time to look at the data and think about what might be going on. Once you have some hypotheses, reveal the block below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fc6df-e876-4b6b-8376-68de485ebf3f",
   "metadata": {},
   "source": [
    "It's clear that throughput is increasing with batch size. This makes sense due to the parallel processing nature of the GPU. A larger batch size means more images can be run through the model in parallel to calculate the loss before back-propagation. This takes advantage of the thousands of cores that are in the GPU. \n",
    "\n",
    "However, throughput does not increase linearly with batch size, and there are diminishing returns as the batch size increases. Eventually, you will have saturated the compute capacity of the GPU. GPUs work effectively when they can spawn tens or hundreds of thousands of threads at once, and for small batch sizes there's not enough to work use all the threads the GPU can execute. Since GPU processing performance depends on hiding latency by having lots of work to do, performance for small batches will be relatively poor, and performance for large enough batches will eventually use all the GPU cores effectively, and the throughput (number of images processed per second) will approach a ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac11505-1aa5-49f9-ad17-e2422a037e68",
   "metadata": {},
   "source": [
    "## Training Performance: Time to Accuracy\n",
    "\n",
    "At this point you might want to pick the highest possible batch size for training to achieve the highest throughput. However, while throughput is an important measurement of the training process, it doesn't indicate how well the model is being trained with respect to its purpose: inference. \n",
    "\n",
    "In our case, the model is only as good as its ability to correctly identify the class of clothing in a given image. Our measurement of this is reported in the validation accuracy, which reflects how effectively the model makes predictions on a separate dataset that we did not train on.\n",
    "\n",
    "Think about how batch size might affect the model's ability to be accurate. Remember that noise introduced with smaller batch sizes is a helpful tool in the training process.\n",
    "\n",
    "In this next exercise, you'll once again adjust batch size, comparing the total training time before a given accuracy is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a10fa8-c362-4e8e-8bea-9cd41b7430c3",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "To implement early stopping, we have to specify a target accuracy as well as a patience value. The patience determines how many epochs should surpass a target accuracy before the training should stop. Sometimes validation accuracy can unexpectedly spike before the network is effectively trained. Maintaining a high accuracy over more than one epoch gives us a better confidence that the network is well-trained, and can generalize effectively.\n",
    "\n",
    "Implement the early stopping feature in `fashion_mnist.py`. Look for `TODO Step 2`. Finally, run the training below with the given target accuracy and patience value. If you get stuck, you can check out `solutions/fashion_mnist_after_step_02.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2352da-2461-49a4-8dd3-974133fd2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --target-accuracy .82 --patience 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdab5c5-f57b-4516-b099-6a87cb65bff8",
   "metadata": {},
   "source": [
    "### Report Total Training Time\n",
    "\n",
    "Now that you've got the training to stop after it reaches a certain accuracy, the next step is to report the total training time so that you can compare runs against each other. Look for `TODO Step 3` in `fashion_mnist.py`. If you get stuck, you can check out `solutions/fashion_mnist_after_step_03.py`.\n",
    "\n",
    "Once you're done, run the file again to test out the functionality. For this exercise it's OK to use a lower target accuracy or to use a lower patience threshold, since we're just making sure we got the code right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3d5ed-b684-477d-8c3e-096e182cde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --target-accuracy FIXME --patience FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93792f99-35a5-4a42-ad12-6b87f3250c08",
   "metadata": {},
   "source": [
    "### Comparing Accuracy with Batch Size\n",
    "\n",
    "You now have a system to compare the effectiveness of batch size in terms of time to a certain accuracy (we suggest between .82 and .85). Try out several batch sizes to see the effects on the validation accuracy.  Notice what happens when your batch size is especially low or high. Consider repeating training with the same batch size one or multiple times to evaluate the consistency of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef42cc-4168-4368-a1c0-885aac71c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fashion_mnist.py --batch-size FIXME --target-accuracy FIXME --patience FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332ce02-d8e4-4329-9cf4-f0cd37a49077",
   "metadata": {},
   "source": [
    "Record and consider your results before revealing the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a66dbe-e860-464f-9966-e6ce0a7eac99",
   "metadata": {},
   "source": [
    "The results you obtained probably pointed you in a couple of general directions. In particular, very small or large batch sizes may not be optimal for convergence (very small batches tend to be far too noisy to converge adequately to the minimum of the loss function, while very large batches tend to diverge in the early part of the training). However, you also probably saw that there's a lot of randomness in the results, and it is hard to generalize very well. That's OK, and it's actually a good thing, because not everything you learn today will apply the same way to every model and dataset. The goal of this course is to build intuition about how to think about the process of neural network optimization, not to learn a set of rules to apply blindly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d93c9a-718d-4121-ac02-c83a2352d5f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this part of the class we have learned:\n",
    "\n",
    "- How to train a more sophisticated and realistic neural network model than the one we used before\n",
    "- How to implement several custom features in PyTorch and measure training performance in terms of both accuracy and throughput\n",
    "- How batch size affects training accuracy for a more realistic model\n",
    "\n",
    "This concludes Lab 1. In Lab 2, we will learn how to extend this training process to multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d728a-08ed-4ec1-b2b9-8180354e5f76",
   "metadata": {},
   "source": [
    "### Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f132270-e260-4b11-af1e-7b5ee96a633d",
   "metadata": {},
   "source": [
    "Before concluding this section please execute the cell below to view the slide presentation that the instructor present at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bcee0c-dc5b-4e2d-a883-5967df0c2977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://view.officeapps.live.com/op/embed.aspx?src=https://developer.download.nvidia.com:443/training/courses/C-MG-01-V3/task1/3-v1.pptx\" width=\"610px\" height=\"367px\" frameborder=\"0\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://view.officeapps.live.com/op/embed.aspx?src=https://developer.download.nvidia.com:443/training/courses/C-MG-01-V3/task1/3-v2.pptx\" width=\"610px\" height=\"367px\" frameborder=\"0\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
